{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aI2cdToavCzD2GnoGhFDfnzK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 167949,
          "status": "ok",
          "timestamp": 1709828309209,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "aI2cdToavCzD2GnoGhFDfnzK",
        "outputId": "e4cd5c42-65b0-4882-80ac-14927a6e3c93",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.colab import auth, files\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import numpy as np\n",
        "\n",
        "# Authenticate to GCS\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Define the GCS bucket\n",
        "bucket_name = 'prd-marketshare'\n",
        "\n",
        "# Set up GCS client\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# Function to get the latest file path from a bucket folder\n",
        "def get_latest_file_path(bucket, folder_path):\n",
        "    blobs = list(bucket.list_blobs(prefix=folder_path))\n",
        "    if not blobs:\n",
        "        return None\n",
        "    latest_blob = max(blobs, key=lambda b: b.time_created)\n",
        "    return f\"gs://{bucket.name}/{latest_blob.name}\"\n",
        "\n",
        "# Function to save files to bucket folders\n",
        "def save_to_bucket(df, bucket, destination_folder, file_name):\n",
        "    # Temporary saving the file locally\n",
        "    local_file_path = f'/tmp/{file_name}'\n",
        "    df.to_csv(local_file_path, index=False)\n",
        "\n",
        "    # Upload the file to the specific folder in GCS\n",
        "    blob = bucket.blob(f'{destination_folder}/{file_name}')\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "\n",
        "    # Download the file (if needed)\n",
        "    #files.download(local_file_path)\n",
        "\n",
        "# Get the paths of the latest files from specified folders\n",
        "ps_file_gcs_path = get_latest_file_path(bucket, '03_power_schedule/02_power_schedule_cleaned/')\n",
        "pihs_vl_file_gcs_path = get_latest_file_path(bucket, '02_value_list/03_pihs_vl_merged')\n",
        "\n",
        "# Read the latest files into DataFrames directly from GCS paths\n",
        "if ps_file_gcs_path:\n",
        "    ps = pd.read_csv(ps_file_gcs_path)\n",
        "    ps = ps[ps['change_tracker'] != 'removed']\n",
        "else:\n",
        "    print(\"No ps file found\")\n",
        "\n",
        "if pihs_vl_file_gcs_path:\n",
        "    pihs_vl = pd.read_csv(pihs_vl_file_gcs_path)\n",
        "else:\n",
        "    print(\"No pihs_vl file found\")\n",
        "\n",
        "\n",
        "# Define columns for exact and range matching\n",
        "exact_match_columns = [\n",
        "    'electrification', 'propulsion_system_subdesign_architecture'\n",
        "]\n",
        "\n",
        "\n",
        "# Explicitly set the specified columns in pihs_vl to float\n",
        "float_columns_pihs_vl = [\n",
        "    'system_voltage_v',\n",
        "    'ap_system_power_kw',\n",
        "    'electric_motor_power_kw'\n",
        "]\n",
        "\n",
        "for col in float_columns_pihs_vl:\n",
        "    pihs_vl[col] = pd.to_numeric(pihs_vl[col], errors='coerce').astype('float')\n",
        "\n",
        "\n",
        "# Function to construct query based on exact matching columns\n",
        "def construct_query(row, exact_match_columns):\n",
        "    query_parts = [f\"{col} == '{row[col]}'\" for col in exact_match_columns if pd.notnull(row[col])]\n",
        "    return ' & '.join(query_parts) if query_parts else None\n",
        "\n",
        "# Initialize a DataFrame to store the constructed queries\n",
        "query_log = pd.DataFrame(columns=['rule_number', 'query'])\n",
        "\n",
        "# Initialize merged DataFrame with all rows from pihs_vl and additional status column\n",
        "merged_columns = pihs_vl.columns.tolist() + ['pihs_vl_ps_status']\n",
        "merged_df = pihs_vl.reindex(columns=merged_columns)\n",
        "merged_df['pihs_vl_ps_status'] = 'unmatched'\n",
        "\n",
        "# Add a marker column in pihs_vl to track if a row has been matched\n",
        "pihs_vl['matched'] = False\n",
        "\n",
        "# Function to calculate power based on given coefficients and values\n",
        "def calculate_power(row, a, b):\n",
        "    x = row['electric_motor_power_kw']\n",
        "    y = row['ap_system_power_kw']\n",
        "    return a * x + b * y\n",
        "\n",
        "# Iterate over each row in ps and apply logic to pihs_vl\n",
        "for index, row in ps.iterrows():\n",
        "    query = construct_query(row, exact_match_columns)\n",
        "    if query:\n",
        "        # Append the constructed query to the query log\n",
        "        query_log = query_log.append({'rule_number': row['row'], 'query': query}, ignore_index=True)\n",
        "\n",
        "        # Find rows in pihs_vl that match the query\n",
        "        matched_rows = pihs_vl.query(query).index\n",
        "        for idx in matched_rows:\n",
        "            if not pihs_vl.at[idx, 'matched']:\n",
        "                # Calculate power for each component and add as a new column in merged_df\n",
        "                for component in ['none_emotor_1', 'none_emotor_2', 'none_emotor_3', 'none_emotor_4', 'none_emotor_5',\n",
        "                                  'none_inverter_1', 'none_inverter_2', 'none_inverter_3', 'none_inverter_4', 'none_inverter_5',\n",
        "                                  'diff_reducer_1', 'diff_reducer_2', 'diff_reducer_3',\n",
        "                                  'diff_reducer_4', 'nodiff_reducer_1', 'nodiff_reducer_2',\n",
        "                                  'nodiff_reducer_3', 'nodiff_reducer_4', 'none_eaxle_1', 'none_eaxle_2',\n",
        "                                  'none_eaxle_3', 'none_eaxle_4', 'none_obc_1', 'none_dcdc_1', 'none_starter_1', 'none_restarter_1', 'none_alternator_1']:\n",
        "                    a = row[f'{component}_a']\n",
        "                    b = row[f'{component}_b']\n",
        "                    merged_df.at[idx, f'power_{component}'] = calculate_power(pihs_vl.loc[idx], a, b)\n",
        "\n",
        "                # Update the status in merged_df and mark the row as matched in pihs_vl\n",
        "                merged_df.loc[idx, 'pihs_vl_ps_status'] = row['row']\n",
        "                pihs_vl.at[idx, 'matched'] = True\n",
        "\n",
        "\n",
        "\n",
        "# Remove all columns from ps in the final merged DataFrame\n",
        "final_columns = ['pihs_vl_ps_status'] + [col for col in merged_df.columns if col.startswith('power_')]\n",
        "final_merged_df = merged_df[final_columns]\n",
        "\n",
        "# Filter merged_df by 'unmatched' status\n",
        "unmerged_df = merged_df[merged_df['pihs_vl_ps_status'] == 'unmatched']\n",
        "\n",
        "# Generate file name for the processed files\n",
        "paris_tz = pytz.timezone('Europe/Paris')\n",
        "current_time = datetime.now(paris_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Save query_log to GCS bucket\n",
        "query_log_file_name = f'pihs_vl_ps_query_log_{current_time}.csv'\n",
        "query_log_folder = '03_power_schedule/05_pihs_vl_ps_query_log'\n",
        "save_to_bucket(query_log, bucket, query_log_folder, query_log_file_name)\n",
        "\n",
        "# Save merged_df to GCS bucket\n",
        "final_merged_df_file_name = f'pihs_vl_ps_merged_{current_time}.csv'\n",
        "final_merged_df_folder = '03_power_schedule/03_pihs_vl_ps_merged'\n",
        "save_to_bucket(merged_df, bucket, final_merged_df_folder, final_merged_df_file_name)\n",
        "\n",
        "# Save merged_df to GCS bucket\n",
        "unmerged_df_file_name = f'pihs_vl_ps_unmerged_{current_time}.csv'\n",
        "unmerged_df_folder = '03_power_schedule/04_pihs_vl_ps_unmerged'\n",
        "save_to_bucket(unmerged_df, bucket, unmerged_df_folder, unmerged_df_file_name)\n",
        "\n",
        "# Print statements to confirm the process\n",
        "print(\"\\nQuery log:\")\n",
        "print(query_log.head())\n",
        "print(\"\\nSample of merged DataFrame:\")\n",
        "print(merged_df.head())\n",
        "\n",
        "# Identify the column with mixed types\n",
        "mixed_type_column = pihs_vl.columns[128]\n",
        "print(f\"Column with mixed types: {mixed_type_column}\")\n",
        "\n",
        "# Inspect the values in this column\n",
        "print(pihs_vl[mixed_type_column].value_counts())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
