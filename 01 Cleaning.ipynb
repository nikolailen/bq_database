{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eNju2vRfup91AhXdZbCNUMcB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 59415,
          "status": "ok",
          "timestamp": 1709809636202,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "eNju2vRfup91AhXdZbCNUMcB",
        "outputId": "7f98e936-672f-403f-8b96-9d416050d086",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.colab import auth, files\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.cloud import storage\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Authenticate to GCS\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Define the GCS bucket\n",
        "bucket_name = 'prd-marketshare'\n",
        "project_id = 'bq-rf5039'\n",
        "\n",
        "# Set up GCS client\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# 1.1 Function to replace cells with a single \"#\" with empty\n",
        "\n",
        "def replace_single_symbol_with_na(df, *column_names):\n",
        "    for column_name in column_names:\n",
        "        if df[column_name].dtype == 'object':  # For non-numeric columns\n",
        "            df[column_name] = df[column_name].apply(lambda x: \"\" if str(x) == '#' else x)\n",
        "        else:  # For numeric columns\n",
        "            df[column_name] = df[column_name].apply(lambda x: np.nan if str(x) == '#' else x)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 1.2 Function to replace commas by dots and convert the column to float\n",
        "\n",
        "def replace_commas_and_convert_to_float(df, *column_names):\n",
        "    for column_name in column_names:\n",
        "        # Ensure the column is of string type\n",
        "        if not isinstance(df[column_name].iloc[0], str):\n",
        "            df[column_name] = df[column_name].astype(str)\n",
        "\n",
        "        # Perform the string replace and then convert to float\n",
        "        df[column_name] = df[column_name].str.replace(',', '.').astype(float)\n",
        "    return df\n",
        "\n",
        "# 1.3 Function to convert dates into proper format\n",
        "\n",
        "def convert_date(value):\n",
        "    # Convert MM.YYYY format\n",
        "    if value.count('.') == 1:\n",
        "        try:\n",
        "            # Parse the month and year\n",
        "            month, year = value.split('.')\n",
        "            last_day = pd.Timestamp(year=int(year), month=int(month), day=1).to_period('M').to_timestamp('M')\n",
        "            return last_day.strftime('%Y-%m-%d')  # Adjusted to desired format\n",
        "        except:\n",
        "            return value\n",
        "\n",
        "    # Convert DD.MM.YYYY format\n",
        "    elif value.count('.') == 2:\n",
        "        try:\n",
        "            return pd.to_datetime(value, format='%d.%m.%Y').strftime('%Y-%m-%d')  # Adjusted to desired format\n",
        "        except:\n",
        "            return value\n",
        "    # Return original if no match\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "def process_dates(df, *column_names):\n",
        "    for column in column_names:\n",
        "        df[column] = df[column].apply(lambda x: \"{:.4f}\".format(x) if not isinstance(x, str) else x)\n",
        "        df[column] = df[column].apply(convert_date)\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "#df = pd.DataFrame({'date_column': ['12.2022', '15.12.2022']})\n",
        "#processed_df = process_dates(df, 'date_column')\n",
        "#print(processed_df)\n",
        "\n",
        "# 1.4 Function to convert values of columns to int\n",
        "\n",
        "def columns_to_int(df, *column_names):\n",
        "    for column_name in column_names:\n",
        "        # Convert column to Pandas nullable integer type\n",
        "        df[column_name] = df[column_name].astype('Int64')\n",
        "    return df\n",
        "\n",
        "# 1.5 Function to lowercase all string columns\n",
        "\n",
        "def lowercase_string_columns(df):\n",
        "    # Identify string columns (object dtype)\n",
        "    string_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Apply lowercase transformation only to string columns\n",
        "    for column in string_columns:\n",
        "        df[column] = df[column].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 1.6 Function to get the latest file from the bucket folder\n",
        "\n",
        "def get_latest_file(bucket, folder_path):\n",
        "    # List all blobs in the specified folder\n",
        "    blobs = bucket.list_blobs(prefix=folder_path)\n",
        "\n",
        "    # Sort the blobs by their creation time (most recent first)\n",
        "    sorted_blobs = sorted(blobs, key=lambda b: b.time_created, reverse=True)\n",
        "\n",
        "    # Return the most recent blob, if available\n",
        "    return sorted_blobs[0] if sorted_blobs else None\n",
        "\n",
        "# 1.7 Function to save the file to proper folder in the bucket\n",
        "\n",
        "def save_to_bucket(df, bucket, destination_folder, file_name):\n",
        "    # Temporary saving the file locally\n",
        "    local_file_path = f'/tmp/{file_name}'\n",
        "    df.to_csv(local_file_path, index=False)\n",
        "\n",
        "    # Upload the file to the specific folder in GCS\n",
        "    blob = bucket.blob(f'{destination_folder}/{file_name}')\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "\n",
        "    # Download the file (if needed)\n",
        "    #files.download(local_file_path)\n",
        "\n",
        "# 1.8 Format column names for BigQuery\n",
        "\n",
        "def format_column_name(column_name):\n",
        "    # Convert to lowercase\n",
        "    formatted_name = column_name.lower()\n",
        "    # Remove periods\n",
        "    formatted_name = formatted_name.replace('.', '')\n",
        "    # Replace colons, hyphens, and spaces with underscore\n",
        "    formatted_name = re.sub(r'[:\\- ]+', '_', formatted_name)\n",
        "    # Delete closing parentheses\n",
        "    formatted_name = formatted_name.replace(')', '')\n",
        "    # Replace multiple consecutive non-alphanumeric characters (now including underscores) with a single underscore\n",
        "    formatted_name = re.sub(r'[_\\s/|()]+', '_', formatted_name)\n",
        "    # Remove any leading/trailing underscores\n",
        "    formatted_name = formatted_name.strip('_')\n",
        "    # Prefix with 'volumes_' if the name starts with a number\n",
        "    if formatted_name[0].isdigit():\n",
        "        formatted_name = 'volumes_' + formatted_name\n",
        "    return formatted_name\n",
        "\n",
        "# 1.9 Delete nan\n",
        "\n",
        "def convert_nan_to_none(df):\n",
        "    # Iterate over the DataFrame and replace NaN with None\n",
        "    for col in df.columns:\n",
        "        df[col] = df[col].apply(lambda x: None if pd.isna(x) else x)\n",
        "    return df\n",
        "\n",
        "# 2 Define a list of configurations\n",
        "file_configs = [\n",
        "    {\n",
        "        'source_folder': '01_pihs/01_pihs_raw/',\n",
        "        'destination_folder': '01_pihs/02_pihs_cleaned',\n",
        "        'file_format': 'excel',\n",
        "        'processing_steps': [\n",
        "            (replace_single_symbol_with_na, ['Transmission Manufacturer', 'Model Code', 'Creation Date|Calendar Year']),\n",
        "            (replace_commas_and_convert_to_float, ['Electric Motor Power (kW)', 'System Voltage (V)', 'AP: Battery Capacity (kWh)', 'AP: System Torque (Nm)', 'Electric Motor Torque (Nm)', 'T: Torque (N.m)']),\n",
        "            (process_dates, ['Engine EOP', 'Vehicle EOP (End of Production)', 'Vehicle SOP (Start of Production)','Creation Date|Calendar Year']),\n",
        "            (columns_to_int, ['2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030', '2031', '2032', '2033', '2034', '2035','Transmission Forward Speed', 'Powertrain ID','Vehicle ID']),\n",
        "            (lowercase_string_columns, None),  # If the function applies to all columns, use None\n",
        "            (format_column_name, None)\n",
        "        ],\n",
        "        'dtypes': {'Global Nameplate': str, 'Transmission Program': str, 'Vehicle Platform': str, 'Vehicle Program': str, 'Engine Platform Value': str, 'T: Program_2': str, 'Mnemonic Vehicle ID': str},\n",
        "        'load_to_bigquery': False  # Add this to specify whether to load to BigQuery\n",
        "    },\n",
        "    {\n",
        "        'source_folder': '02_value_list/01_vl_raw/',\n",
        "        'destination_folder': '02_value_list/02_vl_cleaned',\n",
        "        'file_format': 'csv',\n",
        "        'processing_steps': [\n",
        "            (lowercase_string_columns, None)  # If the function applies to all columns, use None\n",
        "        ],\n",
        "        'dtypes': None,\n",
        "        'load_to_bigquery': False  # Add this to specify whether to load to BigQuery\n",
        "    },\n",
        "    {\n",
        "        'source_folder': '03_power_schedule/01_power_schedule_raw/',\n",
        "        'destination_folder': '03_power_schedule/02_power_schedule_cleaned',\n",
        "        'file_format': 'csv',\n",
        "        'processing_steps': [\n",
        "            (lowercase_string_columns, None)  # If the function applies to all columns, use None\n",
        "        ],\n",
        "        'dtypes': None,\n",
        "        'load_to_bigquery': False  # Add this to specify whether to load to BigQuery\n",
        "    },\n",
        "    {\n",
        "        'source_folder': '04_dictionary/01_emot/01_emot_raw/',\n",
        "        'destination_folder': '04_dictionary/01_emot/02_emot_cleaned',\n",
        "        'file_format': 'csv',\n",
        "        'processing_steps': [\n",
        "            (lowercase_string_columns, None),\n",
        "            (convert_nan_to_none, None) # If the function applies to all columns, use None\n",
        "\n",
        "        ],\n",
        "        'dtypes': {'mnemonic_vehicle_id': str},\n",
        "        'load_to_bigquery': False  # Add this to specify whether to load to BigQuery\n",
        "    },\n",
        "    {\n",
        "        'source_folder': '04_dictionary/02_obcdcdc/01_obcdcdc_raw/',\n",
        "        'destination_folder': '04_dictionary/02_obcdcdc/02_obcdcdc_cleaned',\n",
        "        'file_format': 'csv',\n",
        "        'processing_steps': [\n",
        "            (lowercase_string_columns, None),\n",
        "            (convert_nan_to_none, None) # If the function applies to all columns, use None\n",
        "\n",
        "        ],\n",
        "        'dtypes': None,\n",
        "        'load_to_bigquery': False  # Add this to specify whether to load to BigQuery\n",
        "    },\n",
        "    {\n",
        "        'source_folder': '04_dictionary/03_addressability/01_addressability_raw/',\n",
        "        'destination_folder': '04_dictionary/03_addressability/02_addressability_cleaned',\n",
        "        'file_format': 'csv',\n",
        "        'processing_steps': [\n",
        "            (lowercase_string_columns, None),\n",
        "            (convert_nan_to_none, None) # If the function applies to all columns, use None\n",
        "\n",
        "        ],\n",
        "        'dtypes': None,\n",
        "        'load_to_bigquery': True,  # Set to True for files that need to be loaded into BigQuery\n",
        "        'bigquery_dataset': 'ADDRESSABILITY'  # Specify the BigQuery dataset for this file\n",
        "    },\n",
        "    {\n",
        "        'source_folder': '06_prices/01_prices_raw/',\n",
        "        'destination_folder': '06_prices/02_prices_cleaned',\n",
        "        'file_format': 'csv',\n",
        "        'processing_steps': [\n",
        "            (lowercase_string_columns, None),\n",
        "            (convert_nan_to_none, None) # If the function applies to all columns, use None\n",
        "\n",
        "        ],\n",
        "        'dtypes': {'row': int, 'year': int,\n",
        "                   'system_voltage_v_lower_bound': float, 'system_voltage_v_upper_bound': float,\n",
        "                   'electric_motor_power_kw_lower_bound': float, 'electric_motor_power_kw_upper_bound': float\n",
        "                   },\n",
        "        'load_to_bigquery': True,  # Set to True for files that need to be loaded into BigQuery\n",
        "        'bigquery_dataset': 'PRICES'  # Specify the BigQuery dataset for this file\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "# Function to load CSV from GCS to BigQuery\n",
        "def load_csv_to_bigquery(bucket_name, folder_name, file_name, project_id, dataset_name):\n",
        "    client = bigquery.Client()\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        autodetect=True,\n",
        "        write_disposition='WRITE_APPEND'\n",
        "    )\n",
        "    file_uri = f\"gs://{bucket_name}/{folder_name}/{file_name}\"\n",
        "    table_name = file_name.rsplit('.', 1)[0]\n",
        "    table_id = f\"{project_id}.{dataset_name}.{table_name}\"\n",
        "    load_job = client.load_table_from_uri(file_uri, table_id, job_config=job_config)\n",
        "    load_job.result()\n",
        "    print(f\"Loaded {load_job.output_rows} rows into {table_id}.\")\n",
        "\n",
        "# Process files and load to BigQuery\n",
        "for config in file_configs:\n",
        "    latest_blob = get_latest_file(bucket, config['source_folder'])\n",
        "    if latest_blob:\n",
        "        gcs_path = f'gs://{bucket_name}/{latest_blob.name}'\n",
        "        print(f\"Processing file: {gcs_path}\")\n",
        "\n",
        "        # Determine how to read the file based on its format\n",
        "        if config['file_format'] == 'excel':\n",
        "            df = pd.read_excel(gcs_path, sheet_name='Data', dtype=config['dtypes'] if config['dtypes'] else None)\n",
        "        elif config['file_format'] == 'csv':\n",
        "            df = pd.read_csv(gcs_path, dtype=config['dtypes'] if config['dtypes'] else None)\n",
        "\n",
        "        # Apply specified processing functions to their respective columns\n",
        "        for func, columns in config['processing_steps']:\n",
        "            if func == format_column_name:\n",
        "                df.columns = [func(col) for col in df.columns]\n",
        "            elif columns:\n",
        "                df = func(df, *columns)\n",
        "            else:\n",
        "                df = func(df)\n",
        "\n",
        "        # Generate file name for the processed file\n",
        "        paris_tz = pytz.timezone('Europe/Paris')\n",
        "        current_time = datetime.now(paris_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
        "        original_file_name = os.path.splitext(os.path.basename(latest_blob.name))[0]\n",
        "        processed_file_name = f'{original_file_name}_cleaned_{current_time}.csv'\n",
        "\n",
        "        # Save the processed file to GCS\n",
        "        save_to_bucket(df, bucket, config['destination_folder'], processed_file_name)\n",
        "\n",
        "        # Load the processed file to BigQuery if specified\n",
        "        if config.get('load_to_bigquery', False):\n",
        "            load_csv_to_bigquery(\n",
        "                bucket_name,\n",
        "                config['destination_folder'],\n",
        "                processed_file_name,\n",
        "                project_id,\n",
        "                config['bigquery_dataset']\n",
        "            )\n",
        "    else:\n",
        "        print(f\"No files found in the source folder: {config['source_folder']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
