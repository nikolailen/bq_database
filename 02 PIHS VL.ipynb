{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cFhstZCJNpMYXcTGjqsvo4Zz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 281627,
          "status": "ok",
          "timestamp": 1709223397589,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "cFhstZCJNpMYXcTGjqsvo4Zz",
        "outputId": "579928d4-e48e-457a-daf0-57263ebdf06f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.colab import auth, files\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import numpy as np\n",
        "\n",
        "# Authenticate to GCS\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Define the GCS bucket\n",
        "bucket_name = 'prd-marketshare'\n",
        "\n",
        "# Set up GCS client\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# Function to get the latest file path from a bucket folder\n",
        "def get_latest_file_path(bucket, folder_path):\n",
        "    blobs = list(bucket.list_blobs(prefix=folder_path))\n",
        "    if not blobs:\n",
        "        return None\n",
        "    latest_blob = max(blobs, key=lambda b: b.time_created)\n",
        "    return f\"gs://{bucket.name}/{latest_blob.name}\"\n",
        "\n",
        "# Function to save files to bucket folders\n",
        "def save_to_bucket(df, bucket, destination_folder, file_name):\n",
        "    # Temporary saving the file locally\n",
        "    local_file_path = f'/tmp/{file_name}'\n",
        "    df.to_csv(local_file_path, index=False)\n",
        "\n",
        "    # Upload the file to the specific folder in GCS\n",
        "    blob = bucket.blob(f'{destination_folder}/{file_name}')\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "\n",
        "    # Download the file (if needed)\n",
        "    #files.download(local_file_path)\n",
        "\n",
        "# Get the paths of the latest files from specified folders\n",
        "vl_file_gcs_path = get_latest_file_path(bucket, '02_value_list/02_vl_cleaned/')\n",
        "pihs_file_gcs_path = get_latest_file_path(bucket, '01_pihs/02_pihs_cleaned/')\n",
        "\n",
        "# Read the latest files into DataFrames directly from GCS paths\n",
        "if vl_file_gcs_path:\n",
        "    vl = pd.read_csv(vl_file_gcs_path)\n",
        "    vl = vl[vl['change_tracker'] != 'removed']\n",
        "else:\n",
        "    print(\"No vl file found\")\n",
        "\n",
        "if pihs_file_gcs_path:\n",
        "    pihs = pd.read_csv(pihs_file_gcs_path)\n",
        "else:\n",
        "    print(\"No pihs file found\")\n",
        "\n",
        "\n",
        "# Define columns for exact and range matching\n",
        "exact_match_columns = [\n",
        "    'electrification', 'propulsion_system_subdesign_architecture', 'ap_px_definition'\n",
        "]\n",
        "\n",
        "range_match_columns = [\n",
        "    ('system_voltage_v_lower_bound', 'system_voltage_v_upper_bound', 'system_voltage_v_range_type', 'system_voltage_v')\n",
        "]\n",
        "\n",
        "# Explicitly set the specified columns in vl to float\n",
        "float_columns_vl = [\n",
        "    'system_voltage_v_lower_bound', 'system_voltage_v_upper_bound'\n",
        "]\n",
        "\n",
        "for col in float_columns_vl:\n",
        "    vl[col] = pd.to_numeric(vl[col], errors='coerce').astype('float')\n",
        "\n",
        "# Explicitly set the specified columns in pihs to float\n",
        "float_columns_pihs = [\n",
        "    'system_voltage_v',\n",
        "    'ap_system_power_kw',\n",
        "    'electric_motor_power_kw'\n",
        "]\n",
        "\n",
        "for col in float_columns_pihs:\n",
        "    pihs[col] = pd.to_numeric(pihs[col], errors='coerce').astype('float')\n",
        "\n",
        "# Define the handle_range_match function\n",
        "def handle_range_match(vl_row, lower_bound_col, upper_bound_col, range_type_col, pihs_col):\n",
        "    lower_bound_value = vl_row[lower_bound_col]\n",
        "    upper_bound_value = vl_row[upper_bound_col]\n",
        "    range_type = vl_row[range_type_col]\n",
        "\n",
        "    if pd.isnull(range_type):\n",
        "        return None\n",
        "\n",
        "    # Logic for each range type\n",
        "    if range_type == 'equal':\n",
        "        return f\"({pihs_col} == {lower_bound_value})\" if not pd.isnull(lower_bound_value) else None\n",
        "    elif range_type == 'less than':\n",
        "        return f\"({pihs_col} < {upper_bound_value})\" if not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'greater than':\n",
        "        return f\"({pihs_col} > {lower_bound_value})\" if not pd.isnull(lower_bound_value) else None\n",
        "    elif range_type == 'greater or equal':\n",
        "        return f\"({pihs_col} >= {lower_bound_value})\" if not pd.isnull(lower_bound_value) else None\n",
        "    elif range_type == 'less or equal':\n",
        "        return f\"({pihs_col} <= {upper_bound_value})\" if not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'exclusive-exclusive':\n",
        "        return f\"({pihs_col} > {lower_bound_value}) & ({pihs_col} < {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'exclusive-inclusive':\n",
        "        return f\"({pihs_col} > {lower_bound_value}) & ({pihs_col} <= {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'inclusive-exclusive':\n",
        "        return f\"({pihs_col} >= {lower_bound_value}) & ({pihs_col} < {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'inclusive-inclusive':\n",
        "        return f\"({pihs_col} >= {lower_bound_value}) & ({pihs_col} <= {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'non-inclusive gap':\n",
        "        return f\"({pihs_col} < {lower_bound_value}) | ({pihs_col} > {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "\n",
        "    return None\n",
        "\n",
        "# Define the construct_query function\n",
        "def construct_query(row, exact_match_columns, range_match_columns):\n",
        "    query_parts = [f\"{col} == '{row[col]}'\" for col in exact_match_columns if pd.notnull(row[col])]\n",
        "\n",
        "    for lower_bound_col, upper_bound_col, range_type_col, pihs_col in range_match_columns:\n",
        "        range_query = handle_range_match(row, lower_bound_col, upper_bound_col, range_type_col, pihs_col)\n",
        "        if range_query:\n",
        "            query_parts.append(range_query)\n",
        "\n",
        "    return ' & '.join(query_parts) if query_parts else None\n",
        "\n",
        "# Initialize a DataFrame to store queries\n",
        "query_log = pd.DataFrame(columns=['rule_number', 'query'])\n",
        "\n",
        "# Additional columns from vl to append to merged DataFrame\n",
        "additional_columns_vl = vl.columns[vl.columns.get_loc('system_voltage_v_range_type') + 1:].tolist()\n",
        "\n",
        "# Initialize merged DataFrame with all rows from pihs and additional columns from vl\n",
        "merged_columns = pihs.columns.tolist() + additional_columns_vl + ['pihs_vl_status']\n",
        "merged_df = pihs.reindex(columns=merged_columns)\n",
        "merged_df['pihs_vl_status'] = 'unmatched'\n",
        "\n",
        "# Add a marker column in pihs to track matched status\n",
        "pihs['matched'] = False\n",
        "\n",
        "# Iterate over each rule in vl and apply to pihs, logging queries\n",
        "for index, row in vl.iterrows():\n",
        "    rule_number = row['row']  # Assuming 'row' is a column in vl\n",
        "    query = construct_query(row, exact_match_columns, range_match_columns)\n",
        "    if query:\n",
        "        query_log = query_log.append({'rule_number': rule_number, 'query': query}, ignore_index=True)\n",
        "        matched_rows = pihs.query(query).index\n",
        "        for idx in matched_rows:\n",
        "            if not pihs.at[idx, 'matched']:\n",
        "                merged_df.loc[idx, additional_columns_vl] = row[additional_columns_vl]\n",
        "                merged_df.loc[idx, 'pihs_vl_status'] = row['row']\n",
        "                pihs.at[idx, 'matched'] = True\n",
        "\n",
        "# Filter merged_df by 'unmatched' status\n",
        "unmerged_df = merged_df[merged_df['pihs_vl_status'] == 'unmatched']\n",
        "\n",
        "# Generate file name for the processed files\n",
        "paris_tz = pytz.timezone('Europe/Paris')\n",
        "current_time = datetime.now(paris_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Save query_log to GCS bucket\n",
        "query_log_file_name = f'pihs_vl_query_log_{current_time}.csv'\n",
        "query_log_folder = '02_value_list/05_pihs_vl_query_log'\n",
        "save_to_bucket(query_log, bucket, query_log_folder, query_log_file_name)\n",
        "\n",
        "# Save merged_df to GCS bucket\n",
        "merged_df_file_name = f'pihs_vl_merged_{current_time}.csv'\n",
        "merged_df_folder = '02_value_list/03_pihs_vl_merged'\n",
        "save_to_bucket(merged_df, bucket, merged_df_folder, merged_df_file_name)\n",
        "\n",
        "# Save merged_df to GCS bucket\n",
        "unmerged_df_file_name = f'pihs_vl_unmerged_{current_time}.csv'\n",
        "unmerged_df_folder = '02_value_list/04_pihs_vl_unmerged'\n",
        "save_to_bucket(unmerged_df, bucket, unmerged_df_folder, unmerged_df_file_name)\n",
        "\n",
        "# Print statements to confirm the process\n",
        "print(\"\\nQuery log:\")\n",
        "print(query_log.head())\n",
        "print(\"\\nSample of merged DataFrame:\")\n",
        "print(merged_df.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
