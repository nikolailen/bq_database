{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K2O7l0rt6QowVNZikoypFWKQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 261095,
          "status": "ok",
          "timestamp": 1709820022113,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "K2O7l0rt6QowVNZikoypFWKQ",
        "outputId": "4a97660c-c60f-47cf-d183-b29123ee00cf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.colab import auth, files\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Authenticate to GCS\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Define the GCS bucket\n",
        "bucket_name = 'prd-marketshare'\n",
        "\n",
        "# Set up GCS client\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# Function to get the latest file path from a bucket folder\n",
        "def get_latest_file_path(bucket, folder_path):\n",
        "    blobs = list(bucket.list_blobs(prefix=folder_path))\n",
        "    if not blobs:\n",
        "        return None\n",
        "    latest_blob = max(blobs, key=lambda b: b.time_created)\n",
        "    return f\"gs://{bucket.name}/{latest_blob.name}\"\n",
        "\n",
        "# Function to save files to bucket folders\n",
        "def save_to_bucket(df, bucket, destination_folder, file_name):\n",
        "    # Temporary saving the file locally\n",
        "    local_file_path = f'/tmp/{file_name}'\n",
        "    df.to_csv(local_file_path, index=False)\n",
        "\n",
        "    # Upload the file to the specific folder in GCS\n",
        "    blob = bucket.blob(f'{destination_folder}/{file_name}')\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "\n",
        "    # Download the file (if needed)\n",
        "    #files.download(local_file_path)\n",
        "\n",
        "# Get the paths of the latest files from specified folders\n",
        "source_gcs_path = get_latest_file_path(bucket, '04_dictionary/08_pihs_vl_ps_sd_obc_merged')\n",
        "\n",
        "# Read the latest files into DataFrames directly from GCS paths\n",
        "if source_gcs_path:\n",
        "    df = pd.read_csv(source_gcs_path)\n",
        "else:\n",
        "    print(\"No source file found\")\n",
        "\n",
        "\n",
        "# Columns to parse through\n",
        "parse_cols = ['hv_bsg_emotor_1', 'hv_bsg_emotor_2', 'hv_bsg_emotor_3', 'hv_bsg_emotor_4', 'hv_bsg_emotor_5',\n",
        "'hv_cmg_emotor_1', 'hv_cmg_emotor_2', 'hv_cmg_emotor_3', 'hv_cmg_emotor_4', 'hv_cmg_emotor_5',\n",
        "'hv_gmg_emotor_1', 'hv_gmg_emotor_2', 'hv_gmg_emotor_3', 'hv_gmg_emotor_4', 'hv_gmg_emotor_5',\n",
        "'hv_edrive_emotor_1', 'hv_edrive_emotor_2', 'hv_edrive_emotor_3', 'hv_edrive_emotor_4', 'hv_edrive_emotor_5',\n",
        "'hv_hub_emotor_1', 'hv_hub_emotor_2', 'hv_hub_emotor_3', 'hv_hub_emotor_4', 'hv_hub_emotor_5',\n",
        "'mv_bsg_emotor_1', 'mv_bsg_emotor_2', 'mv_cmg_emotor_1', 'mv_cmg_emotor_2', 'mv_gmg_emotor_1', 'mv_gmg_emotor_2',\n",
        "'mv_edrive_emotor_1', 'mv_edrive_emotor_2', 'hv_diff_reducer_1', 'hv_diff_reducer_2', 'hv_diff_reducer_3',\n",
        "'hv_nodiff_reducer_1', 'hv_nodiff_reducer_2', 'hv_nodiff_reducer_3', 'hv_nodiff_reducer_4',\n",
        "'lv_none_starter_1', 'lv_none_restarter_1', 'lv_none_alternator_1', 'lv_bsg_emotor_1', 'mv_none_eaxle_1',\n",
        "'mv_none_eaxle_2', 'mv_none_inverter_1', 'mv_none_inverter_2', 'mv_none_dcdc_1', 'mv_none_obc_1',\n",
        "'hv_none_eaxle_1', 'hv_none_eaxle_2', 'hv_none_eaxle_3', 'hv_none_inverter_1', 'hv_none_inverter_2',\n",
        "'hv_none_inverter_3', 'hv_none_inverter_4', 'hv_none_obc_1', 'hv_none_dcdc_1'\n",
        "]\n",
        "\n",
        "# Columns for power and supplier\n",
        "power_cols = ['power_none_emotor_1', 'power_none_emotor_2', 'power_none_emotor_3', 'power_none_emotor_4', 'power_none_emotor_5',\n",
        "'power_none_inverter_1', 'power_none_inverter_2', 'power_none_inverter_3', 'power_none_inverter_4', 'power_none_inverter_5',\n",
        "'power_diff_reducer_1', 'power_diff_reducer_2', 'power_diff_reducer_3', 'power_diff_reducer_4',\n",
        "'power_nodiff_reducer_1', 'power_nodiff_reducer_2', 'power_nodiff_reducer_3', 'power_nodiff_reducer_4',\n",
        "'power_none_eaxle_1', 'power_none_eaxle_2', 'power_none_eaxle_3', 'power_none_eaxle_4', 'power_none_obc_1',\t'power_none_dcdc_1',\t'power_none_starter_1',\t'power_none_restarter_1',\t'power_none_alternator_1'\n",
        "]\n",
        "\n",
        "supplier_cols = ['supplier_none_emotor_1', 'supplier_none_emotor_2', 'supplier_none_emotor_3', 'supplier_none_emotor_4',\n",
        "'supplier_none_inverter_1', 'supplier_none_inverter_2', 'supplier_none_inverter_3', 'supplier_none_inverter_4',\n",
        "'supplier_none_eaxle_1', 'supplier_none_eaxle_2', 'supplier_none_obc_1',\t'supplier_none_dcdc_1'\n",
        "]\n",
        "\n",
        "def decompose_name(col_name):\n",
        "    parts = col_name.split('_')\n",
        "    return {\n",
        "        'product_voltage_type': parts[0],\n",
        "        'product_subtype': None if parts[1] == 'none' else parts[1],\n",
        "        'product_type': parts[2],\n",
        "        'product_level': parts[-1]\n",
        "    }\n",
        "\n",
        "def match_column(parsed_name, target_cols):\n",
        "    for target_col in target_cols:\n",
        "        target_parts = target_col.split('_')\n",
        "        if (parsed_name['product_subtype'] == target_parts[1] or target_parts[1] == 'none') and \\\n",
        "           parsed_name['product_type'] == target_parts[2] and \\\n",
        "           parsed_name['product_level'] == target_parts[-1]:\n",
        "            return target_col\n",
        "    return None\n",
        "\n",
        "\n",
        "new_rows = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    for col in parse_cols:\n",
        "        if row[col] != 0 and pd.notna(row[col]):\n",
        "            parsed_name = decompose_name(col)\n",
        "            power_col = match_column(parsed_name, power_cols)\n",
        "            supplier_col = match_column(parsed_name, supplier_cols)\n",
        "\n",
        "            new_row = {\n",
        "                'product_vl': col,\n",
        "                'product_voltage_type': parsed_name['product_voltage_type'],\n",
        "                'product_subtype': parsed_name['product_subtype'],\n",
        "                'product_type': parsed_name['product_type'],\n",
        "                'product_level': parsed_name['product_level'],\n",
        "                'product_power': row[power_col] if power_col else None,\n",
        "                'product_supplier': row[supplier_col] if supplier_col else None\n",
        "            }\n",
        "            # Add original row data to the new row\n",
        "            for original_col in df.columns:\n",
        "                new_row[original_col] = row[original_col]\n",
        "\n",
        "            new_rows.append(new_row)\n",
        "\n",
        "new_df = pd.DataFrame(new_rows)\n",
        "\n",
        "transformed_all_columns = new_df.copy()\n",
        "\n",
        "# Removing unnecessary columns\n",
        "cols_to_remove = ['product_vl'] + parse_cols + power_cols + supplier_cols\n",
        "new_df.drop(columns=cols_to_remove, inplace=True)\n",
        "print(f\"Number of rows after transformation: {len(new_df)}\")  # Print row count after melting\n",
        "\n",
        "\n",
        "# Define the columns for volumes\n",
        "volume_columns = [\n",
        "    'volumes_2020', 'volumes_2021', 'volumes_2022', 'volumes_2023',\n",
        "    'volumes_2024', 'volumes_2025', 'volumes_2026', 'volumes_2027',\n",
        "    'volumes_2028', 'volumes_2029', 'volumes_2030', 'volumes_2031',\n",
        "    'volumes_2032', 'volumes_2033', 'volumes_2034', 'volumes_2035'\n",
        "]\n",
        "\n",
        "# Melt the DataFrame\n",
        "melted_df = new_df.melt(id_vars=[col for col in new_df.columns if col not in volume_columns],\n",
        "                    value_vars=volume_columns,\n",
        "                    var_name='year',\n",
        "                    value_name='volume')\n",
        "print(f\"Number of rows after melting: {len(melted_df)}\")  # Print row count after melting\n",
        "\n",
        "# Remove rows with NaN or zero in 'volume'\n",
        "melted_df = melted_df[melted_df['volume'].notna() & (melted_df['volume'] != 0)]\n",
        "print(f\"Number of rows after removing NaNs and zeros in 'volume': {len(melted_df)}\")  # Print row count after filtering\n",
        "\n",
        "# Convert 'year' to integer by extracting the year part\n",
        "melted_df['year'] = melted_df['year'].apply(lambda x: int(x.split('_')[1]))\n",
        "\n",
        "# Define custom order of columns\n",
        "custom_order = [\n",
        "'pihs_vl_status', 'pihs_vl_ps_status', 'pihs_vl_ps_sd_status', 'pihs_vl_ps_sd_obc_status', 'powertrain_id', 'product_voltage_type', 'product_subtype', 'product_type', 'product_level', 'product_power', 'product_supplier', 'year', 'volume', 'lv_hv', 'sales_group', 'vehicle_design_parent', 'solution_owner_design_parent', 'vehicle_platform', 'global_nameplate', 'vehicle_program', 'electrification', 'propulsion_system_subdesign', 'propulsion_system_subdesign_architecture', 'ap_px_definition', 'system_voltage_v', 'ap_system_power_kw', 'ap_system_torque_nm', 'electric_motor_power_kw', 'electric_motor_torque_nm', 'vehicle_sub_region', 'drive_type', 'engine_eop', 'engine_fuel_type', 'layout', 'engine_manufacturer', 'engine_platform', 'transmission_design', 'transmission_sub_design', 'transmission_forward_speed', 'transmission_manufacturer', 'transmission_program', 't_torque_nm', 't_design_2', 't_sub_design_2', 't_forward_speeds_2', 't_program_2', 'country', 'vehicle_eop_end_of_production', 'global_sales_segment', 'global_sales_sub_segment', 'gvw_class', 'vehicle_manufacturer', 'mnemonic_vehicle_id', 'production_brand', 'vehicle_production_plant', 'production_type', 'vehicle_ihs_region', 'sales_parent', 'vehicle_sop_start_of_production', 'global_make', 'transmission_design', 'battery_type', 'ap_battery_capacity_kwh', 'vehicle_id', 'model_code', 'creation_date_calendar_year', 'primary_eaxle_motor_technology', 'secondary_eaxle_motor_technology', 'primary_eaxle_layout', 'secondary_eaxle_layout', 'switching_techno_inverter_1', 'switching_techno_inverter_2', 'switching_techno_inverter_3', 'switching_techno_inverter_4', 'x_in_1_edrive_nb_of_functions', 'x_in_1_edrive_functions_description', 'x_in_1_edrive_emot', 'x_in_1_edrive_inv', 'x_in_1_edrive_red', 'x_in_1_edrive_obc', 'x_in_1_edrive_dcdc', 'x_in_1_edrive_pdu', 'x_in_1_edrive_vcu', 'x_in_1_edrive_bms', 'x_in_1_edrive_edc', 'x_in_1_edrive_ptc', 'x_in_1_edrive_dcac', 'active_parts_emotor_1', 'active_parts_emotor_2', 'active_parts_emotor_3', 'active_parts_emotor_4', 'active_parts_inverter_1', 'active_parts_inverter_2'\n",
        "\n",
        "]\n",
        "\n",
        "# Reorder columns based on custom order\n",
        "# Use .reindex() to ensure only existing columns in melted_df are selected\n",
        "melted_df = melted_df.reindex(columns=custom_order)\n",
        "print(f\"Number of rows after reordering columns: {len(melted_df)}\")  # Print row count after reordering\n",
        "\n",
        "# Save or use the new DataFrame\n",
        "melted_df.to_csv('reshaped_data.csv', index=False)\n",
        "\n",
        "# Reload the CSV and check the number of rows\n",
        "reloaded_df = pd.read_csv('reshaped_data.csv')\n",
        "print(\"Number of rows after reloading:\", len(reloaded_df))\n",
        "\n",
        "# Generate file name for the processed files\n",
        "paris_tz = pytz.timezone('Europe/Paris')\n",
        "current_time = datetime.now(paris_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Save transformed_all_columns to GCS bucket\n",
        "transformed_all_columns_file_name = f'transformed_all_columns_{current_time}.csv'\n",
        "transformed_all_columns_folder = '05_transformation/01_transformed_all_columns'\n",
        "save_to_bucket(transformed_all_columns, bucket, transformed_all_columns_folder, transformed_all_columns_file_name)\n",
        "\n",
        "# Save transformed_dropped_columns to GCS bucket\n",
        "transformed_dropped_columns_file_name = f'transformed_dropped_columns_{current_time}.csv'\n",
        "transformed_dropped_columns_folder = '05_transformation/02_transformed_dropped_columns'\n",
        "save_to_bucket(new_df, bucket, transformed_dropped_columns_folder, transformed_dropped_columns_file_name)\n",
        "\n",
        "# Save transformed_melted to GCS bucket\n",
        "transformed_melted_file_name = f'transformed_melted_{current_time}.csv'\n",
        "transformed_melted_folder = '05_transformation/03_transformed_melted'\n",
        "save_to_bucket(melted_df, bucket, transformed_melted_folder, transformed_melted_file_name)\n",
        "\n",
        "# Function to load CSV from GCS to BigQuery\n",
        "def load_csv_to_bigquery(bucket, transformed_melted_folder, transformed_melted_file_name, project_id, dataset_name):\n",
        "    # BigQuery client\n",
        "    client = bigquery.Client()\n",
        "\n",
        "    # Configuration for the load job\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,  # Assuming your CSV has a header row\n",
        "        autodetect=True,  # Auto-detect schema\n",
        "        write_disposition='WRITE_APPEND'  # Append to table if it exists, otherwise create a new one\n",
        "    )\n",
        "\n",
        "    # Construct GCS URI\n",
        "    file_uri = f\"gs://{bucket}/{transformed_melted_folder}/{transformed_melted_file_name}\"\n",
        "\n",
        "    # Remove '.csv' from file name for the BigQuery table name\n",
        "    table_name = transformed_melted_file_name.rsplit('.', 1)[0]\n",
        "\n",
        "    # BigQuery table ID\n",
        "    table_id = f\"{project_id}.{dataset_name}.{table_name}\"\n",
        "\n",
        "    # Load data from GCS to BigQuery\n",
        "    load_job = client.load_table_from_uri(file_uri, table_id, job_config=job_config)\n",
        "    load_job.result()\n",
        "\n",
        "    print(f\"Loaded {load_job.output_rows} rows into {table_id}.\")\n",
        "\n",
        "# Load only the transformed_melted CSV to BigQuery\n",
        "load_csv_to_bigquery(\n",
        "    bucket_name,\n",
        "    transformed_melted_folder,\n",
        "    transformed_melted_file_name,\n",
        "    'bq-rf5039',\n",
        "    'MELTED'\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
