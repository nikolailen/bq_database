{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1AQ8oY6CWcRpgXqmnQr9wQnu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 51541,
          "status": "ok",
          "timestamp": 1709290376411,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "1AQ8oY6CWcRpgXqmnQr9wQnu",
        "outputId": "9b4da208-c1ab-49a9-fd16-758501a6b788",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.colab import auth, files\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import numpy as np\n",
        "\n",
        "# Authenticate to GCS\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Define the GCS bucket\n",
        "bucket_name = 'prd-marketshare'\n",
        "\n",
        "# Set up GCS client\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# Function to get the latest file path from a bucket folder\n",
        "def get_latest_file_path(bucket, folder_path):\n",
        "    blobs = list(bucket.list_blobs(prefix=folder_path))\n",
        "    if not blobs:\n",
        "        return None\n",
        "    latest_blob = max(blobs, key=lambda b: b.time_created)\n",
        "    return f\"gs://{bucket.name}/{latest_blob.name}\"\n",
        "\n",
        "# Function to save files to bucket folders\n",
        "def save_to_bucket(df, bucket, destination_folder, file_name):\n",
        "    # Temporary saving the file locally\n",
        "    local_file_path = f'/tmp/{file_name}'\n",
        "    df.to_csv(local_file_path, index=False)\n",
        "\n",
        "    # Upload the file to the specific folder in GCS\n",
        "    blob = bucket.blob(f'{destination_folder}/{file_name}')\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "\n",
        "    # Download the file (if needed)\n",
        "    #files.download(local_file_path)\n",
        "\n",
        "# Get the paths of the latest files from specified folders\n",
        "sd_file_gcs_path = get_latest_file_path(bucket, '04_dictionary/01_emot/02_emot_cleaned/')\n",
        "pihs_vl_ps_file_gcs_path = get_latest_file_path(bucket, '03_power_schedule/03_pihs_vl_ps_merged/')\n",
        "\n",
        "# Read the latest files into DataFrames directly from GCS paths\n",
        "# Read the latest files into DataFrames directly from GCS paths\n",
        "if sd_file_gcs_path:\n",
        "    sd = pd.read_csv(sd_file_gcs_path, dtype={'mnemonic_vehicle_id': str})\n",
        "    sd['mnemonic_vehicle_id'] = sd['mnemonic_vehicle_id'].replace({np.nan: None})\n",
        "    sd = sd[sd['change_tracker'] != 'removed']\n",
        "else:\n",
        "    print(\"No sd file found\")\n",
        "\n",
        "if pihs_vl_ps_file_gcs_path:\n",
        "    pihs_vl_ps = pd.read_csv(pihs_vl_ps_file_gcs_path, dtype={'mnemonic_vehicle_id': str})\n",
        "    pihs_vl_ps['mnemonic_vehicle_id'] = pihs_vl_ps['mnemonic_vehicle_id'].replace({np.nan: None})\n",
        "else:\n",
        "    print(\"No pihs_vl_ps file found\")\n",
        "\n",
        "\n",
        "\n",
        "# Define columns for exact and range matching\n",
        "exact_match_columns = [\n",
        "    'sales_group', 'vehicle_design_parent', 'vehicle_platform', 'global_nameplate',\n",
        "    'electrification', 'propulsion_system_subdesign_architecture', 'ap_px_definition',\n",
        "    'drive_type', 'transmission_design', 'transmission_program', 'global_sales_sub_segment',\n",
        "    'vehicle_manufacturer', 'production_brand', 'vehicle_ihs_region', 'mnemonic_vehicle_id'\n",
        "]\n",
        "\n",
        "range_match_columns = [\n",
        "    ('system_voltage_v_lower_bound', 'system_voltage_v_upper_bound', 'system_voltage_v_range_type', 'system_voltage_v'),\n",
        "    ('ap_system_power_kw_lower_bound', 'ap_system_power_kw_upper_bound', 'ap_system_power_kw_range_type', 'ap_system_power_kw'),\n",
        "    ('electric_motor_power_kw_lower_bound', 'electric_motor_power_kw_upper_bound', 'electric_motor_power_kw_range_type', 'electric_motor_power_kw'),\n",
        "    ('power_emotor2_lower_bound',\t'power_emotor2_upper_bound',\t'power_emotor2_range_type', 'power_none_emotor_2'),\n",
        "    ('vehicle_sop_start_of_production_lower_bound',\t'vehicle_sop_start_of_production_upper_bound',\t'vehicle_sop_start_of_production_range_type', 'vehicle_sop_start_of_production')\n",
        "]\n",
        "\n",
        "# Convert string columns to lowercase for case-insensitive matching\n",
        "#for col in exact_match_columns:\n",
        "#    if sd[col].dtype == 'object':\n",
        "#        sd[col] = sd[col].str.lower()\n",
        "#    if pihs_vl_ps[col].dtype == 'object':\n",
        "#       pihs_vl_ps[col] = pihs_vl_ps[col].str.lower()\n",
        "\n",
        "# Explicitly set the specified columns in sd to float\n",
        "float_columns_sd = [\n",
        "    'system_voltage_v_lower_bound', 'system_voltage_v_upper_bound',\n",
        "    'ap_system_power_kw_lower_bound', 'ap_system_power_kw_upper_bound',\n",
        "    'electric_motor_power_kw_lower_bound', 'electric_motor_power_kw_upper_bound',\n",
        "    'power_emotor2_lower_bound',\t'power_emotor2_upper_bound'\n",
        "]\n",
        "\n",
        "for col in float_columns_sd:\n",
        "    sd[col] = pd.to_numeric(sd[col], errors='coerce').astype('float')\n",
        "\n",
        "# Explicitly set the specified columns in pihs_vl_ps to float\n",
        "float_columns_pihs_vl_ps = [\n",
        "    'system_voltage_v',\n",
        "    'ap_system_power_kw',\n",
        "    'electric_motor_power_kw',\n",
        "    'power_none_emotor_2'\n",
        "\n",
        "]\n",
        "\n",
        "for col in float_columns_pihs_vl_ps:\n",
        "    pihs_vl_ps[col] = pd.to_numeric(pihs_vl_ps[col], errors='coerce').astype('float')\n",
        "\n",
        "# Sort sd by the number of non-null values in descending order\n",
        "\n",
        "## Extracting lower and upper bounds, and range type column names from range_match_columns\n",
        "range_bound_columns = [col for rmc in range_match_columns for col in rmc[:3]]  # First three elements in each tuple\n",
        "\n",
        "## Combine with exact_match_columns\n",
        "relevant_columns = exact_match_columns + range_bound_columns\n",
        "\n",
        "## Count non-null values in relevant columns only\n",
        "non_null_counts = sd[relevant_columns].notnull().sum(axis=1)\n",
        "\n",
        "## Sort sd by the non-null counts in descending order\n",
        "sd = sd.loc[non_null_counts.sort_values(ascending=False).index]\n",
        "\n",
        "sorted_sd = sd\n",
        "\n",
        "def handle_range_match(sd_row, lower_bound_col, upper_bound_col, range_type_col, pihs_vl_ps_col):\n",
        "    lower_bound_value = sd_row[lower_bound_col]\n",
        "    upper_bound_value = sd_row[upper_bound_col]\n",
        "    range_type = sd_row[range_type_col]\n",
        "\n",
        "    # Function to format value based on its type (date or numeric)\n",
        "    def format_value_for_query(value, is_date):\n",
        "        if pd.notnull(value):\n",
        "            if is_date:\n",
        "                try:\n",
        "                    # Convert the value to datetime and format as a string in 'YYYY-MM-DD' format\n",
        "                    date_val = pd.to_datetime(value).strftime('%Y-%m-%d')\n",
        "\n",
        "                    return f\"'{date_val}'\"  # Enclose the date in quotes\n",
        "\n",
        "                except ValueError:\n",
        "                    print(f\"ValueError: Unable to convert {value} to a date.\")\n",
        "                    return None\n",
        "            else:\n",
        "                # Return non-date values as is\n",
        "                return value\n",
        "        return None\n",
        "\n",
        "    # Check if the columns are date type\n",
        "    is_date = \"_start_of_production\" in lower_bound_col or \"_start_of_production\" in upper_bound_col\n",
        "\n",
        "\n",
        "    lower_bound_value = format_value_for_query(lower_bound_value, is_date)\n",
        "    upper_bound_value = format_value_for_query(upper_bound_value, is_date)\n",
        "\n",
        "    # Logic for each range type\n",
        "    if range_type == 'equal':\n",
        "        return f\"({pihs_vl_ps_col} == {lower_bound_value})\" if not pd.isnull(lower_bound_value) else None\n",
        "    elif range_type == 'less than':\n",
        "        return f\"({pihs_vl_ps_col} < {upper_bound_value})\" if not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'greater than':\n",
        "        return f\"({pihs_vl_ps_col} > {lower_bound_value})\" if not pd.isnull(lower_bound_value) else None\n",
        "    elif range_type == 'greater or equal':\n",
        "        return f\"({pihs_vl_ps_col} >= {lower_bound_value})\" if not pd.isnull(lower_bound_value) else None\n",
        "    elif range_type == 'less or equal':\n",
        "        return f\"({pihs_vl_ps_col} <= {upper_bound_value})\" if not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'exclusive-exclusive':\n",
        "        return f\"({pihs_vl_ps_col} > {lower_bound_value}) & ({pihs_vl_ps_col} < {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'exclusive-inclusive':\n",
        "        return f\"({pihs_vl_ps_col} > {lower_bound_value}) & ({pihs_vl_ps_col} <= {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'inclusive-exclusive':\n",
        "        return f\"({pihs_vl_ps_col} >= {lower_bound_value}) & ({pihs_vl_ps_col} < {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'inclusive-inclusive':\n",
        "        return f\"({pihs_vl_ps_col} >= {lower_bound_value}) & ({pihs_vl_ps_col} <= {upper_bound_value})\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "    elif range_type == 'non-inclusive gap':\n",
        "        return f\"(({pihs_vl_ps_col} < {lower_bound_value}) | ({pihs_vl_ps_col} > {upper_bound_value}))\" if not pd.isnull(lower_bound_value) and not pd.isnull(upper_bound_value) else None\n",
        "\n",
        "    return None\n",
        "\n",
        "# Define the construct_query function\n",
        "def construct_query(row, exact_match_columns, range_match_columns):\n",
        "    query_parts = [f\"{col} == '{row[col]}'\" for col in exact_match_columns if pd.notnull(row[col])]\n",
        "\n",
        "    for lower_bound_col, upper_bound_col, range_type_col, pihs_vl_ps_col in range_match_columns:\n",
        "        range_query = handle_range_match(row, lower_bound_col, upper_bound_col, range_type_col, pihs_vl_ps_col)\n",
        "        if range_query:\n",
        "            query_parts.append(range_query)\n",
        "\n",
        "    return ' & '.join(query_parts) if query_parts else None\n",
        "\n",
        "# Initialize a DataFrame to store queries\n",
        "query_log = pd.DataFrame(columns=['rule_number', 'query'])\n",
        "\n",
        "# Additional columns from sd to append to merged DataFrame\n",
        "additional_columns_sd = sd.columns[sd.columns.get_loc('nur_of_eaxle') + 1:].tolist()\n",
        "\n",
        "# Initialize merged DataFrame with all rows from pihs_vl_ps and additional columns from sd\n",
        "merged_columns = pihs_vl_ps.columns.tolist() + additional_columns_sd + ['pihs_vl_ps_sd_status']\n",
        "merged_df = pihs_vl_ps.reindex(columns=merged_columns)\n",
        "merged_df['pihs_vl_ps_sd_status'] = 'unmatched'\n",
        "\n",
        "# Add a marker column in pihs_vl_ps to track matched pihs_vl_ps_sd_status\n",
        "pihs_vl_ps['matched'] = False\n",
        "\n",
        "# Iterate over each rule in sd and apply to pihs_vl_ps, logging queries\n",
        "for index, row in sd.iterrows():\n",
        "    rule_number = row['row']  # Assuming 'row' is a column in sd\n",
        "    query = construct_query(row, exact_match_columns, range_match_columns)\n",
        "    if query:\n",
        "        query_log = query_log.append({'rule_number': rule_number, 'query': query}, ignore_index=True)\n",
        "        print(query)\n",
        "        matched_rows = pihs_vl_ps.query(query).index\n",
        "        for idx in matched_rows:\n",
        "            if not pihs_vl_ps.at[idx, 'matched']:\n",
        "                merged_df.loc[idx, additional_columns_sd] = row[additional_columns_sd]\n",
        "                merged_df.loc[idx, 'pihs_vl_ps_sd_status'] = row['row']\n",
        "                pihs_vl_ps.at[idx, 'matched'] = True\n",
        "\n",
        "# Filter merged_df by 'unmatched' pihs_vl_ps_sd_status\n",
        "unmerged_df = merged_df[merged_df['pihs_vl_ps_sd_status'] == 'unmatched']\n",
        "\n",
        "# Generate file name for the processed files\n",
        "paris_tz = pytz.timezone('Europe/Paris')\n",
        "current_time = datetime.now(paris_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Save query_log to GCS bucket\n",
        "query_log_file_name = f'pihs_vl_ps_sd_query_log_{current_time}.csv'\n",
        "query_log_folder = '04_dictionary/07_pihs_vl_ps_sd_query_log'\n",
        "save_to_bucket(query_log, bucket, query_log_folder, query_log_file_name)\n",
        "\n",
        "# Save merged_df to GCS bucket\n",
        "merged_df_file_name = f'pihs_vl_ps_sd_merged_{current_time}.csv'\n",
        "merged_df_folder = '04_dictionary/04_pihs_vl_ps_sd_merged'\n",
        "save_to_bucket(merged_df, bucket, merged_df_folder, merged_df_file_name)\n",
        "\n",
        "# Save unmerged_df to GCS bucket\n",
        "unmerged_df_file_name = f'pihs_vl_ps_sd_unmerged_{current_time}.csv'\n",
        "unmerged_df_folder = '04_dictionary/05_pihs_vl_ps_sd_unmerged'\n",
        "save_to_bucket(unmerged_df, bucket, unmerged_df_folder, unmerged_df_file_name)\n",
        "\n",
        "# Save sorted_sd to GCS bucket\n",
        "sorted_sd_file_name = f'pihs_vl_ps_sd_sorted_sd_{current_time}.csv'\n",
        "sorted_sd_folder = '04_dictionary/06_pihs_vl_ps_sd_sorted_sd'\n",
        "save_to_bucket(sorted_sd, bucket, sorted_sd_folder, sorted_sd_file_name)\n",
        "\n",
        "# Print statements to confirm the process\n",
        "print(\"\\nQuery log:\")\n",
        "print(query_log.head())\n",
        "print(\"\\nSample of merged DataFrame:\")\n",
        "print(merged_df.head())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
